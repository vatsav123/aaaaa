{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"From Scratch Implementation","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom keras.datasets import mnist\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\n","metadata":{"execution":{"iopub.status.busy":"2024-03-24T15:25:17.594492Z","iopub.execute_input":"2024-03-24T15:25:17.595076Z","iopub.status.idle":"2024-03-24T15:25:17.599634Z","shell.execute_reply.started":"2024-03-24T15:25:17.595049Z","shell.execute_reply":"2024-03-24T15:25:17.598630Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Load the MNIST dataset\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n\n# Preprocessing: Flatten and normalize the images\nX_train = X_train.reshape(X_train.shape[0], -1) / 255.0\nX_test = X_test.reshape(X_test.shape[0], -1) / 255.0","metadata":{"execution":{"iopub.status.busy":"2024-03-24T15:25:22.639205Z","iopub.execute_input":"2024-03-24T15:25:22.639605Z","iopub.status.idle":"2024-03-24T15:25:23.170975Z","shell.execute_reply.started":"2024-03-24T15:25:22.639573Z","shell.execute_reply":"2024-03-24T15:25:23.170170Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","output_type":"stream"}]},{"cell_type":"code","source":"# One-hot encode the labels\none_hot_encoder = OneHotEncoder(sparse=False)\ny_train_one_hot = one_hot_encoder.fit_transform(y_train.reshape(-1, 1))\ny_test_one_hot = one_hot_encoder.transform(y_test.reshape(-1, 1))\n\n# Add bias term to the input\nX_train = np.hstack((X_train, np.ones((X_train.shape[0], 1))))\nX_test = np.hstack((X_test, np.ones((X_test.shape[0], 1))))","metadata":{"execution":{"iopub.status.busy":"2024-03-24T15:33:50.308021Z","iopub.execute_input":"2024-03-24T15:33:50.308413Z","iopub.status.idle":"2024-03-24T15:33:50.453374Z","shell.execute_reply.started":"2024-03-24T15:33:50.308382Z","shell.execute_reply":"2024-03-24T15:33:50.452498Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# Neural Network Implementation\nclass NeuralNetwork:\n    def __init__(self, input_size, hidden_size, output_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        \n        # Initialize weights\n        self.W1 = np.random.randn(self.input_size, self.hidden_size) * 0.01\n        self.W2 = np.random.randn(self.hidden_size, self.output_size) * 0.01\n    \n    def softmax(self, x):\n        exp_scores = np.exp(x - np.max(x, axis=1, keepdims=True))\n        return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n    \n    def forward(self, X):\n        self.z2 = np.dot(X, self.W1)\n        self.a2 = np.tanh(self.z2)\n        self.z3 = np.dot(self.a2, self.W2)\n        self.probs = self.softmax(self.z3)\n        return self.probs\n    \n    def backward(self, X, y, learning_rate):\n        m = X.shape[0]\n        delta3 = self.probs - y\n        dW2 = (1 / m) * np.dot(self.a2.T, delta3)\n        delta2 = np.dot(delta3, self.W2.T) * (1 - np.power(self.a2, 2))\n        dW1 = (1 / m) * np.dot(X.T, delta2)\n        \n        # Update weights\n        self.W1 -= learning_rate * dW1\n        self.W2 -= learning_rate * dW2\n    \n    def train(self, X, y, learning_rate=0.01, epochs=200):# epochs 200\n        for epoch in range(epochs):\n            # Forward propagation\n            probs = self.forward(X)\n            # Backpropagation\n            self.backward(X, y, learning_rate)\n            if epoch % 5 == 0:\n                loss = self.calculate_loss(X, y)\n                print(f\"Epoch {epoch}: Loss {loss}\")\n    \n    def calculate_loss(self, X, y):\n        m = X.shape[0]\n        log_probs = -np.log(self.probs[range(m), np.argmax(y, axis=1)])\n        loss = np.sum(log_probs) / m\n        return loss","metadata":{"execution":{"iopub.status.busy":"2024-03-24T15:37:21.297665Z","iopub.execute_input":"2024-03-24T15:37:21.298467Z","iopub.status.idle":"2024-03-24T15:37:21.310647Z","shell.execute_reply.started":"2024-03-24T15:37:21.298435Z","shell.execute_reply":"2024-03-24T15:37:21.309753Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Define neural network parameters\ninput_size = X_train.shape[1]\nhidden_size = 64\noutput_size = 10\n","metadata":{"execution":{"iopub.status.busy":"2024-03-24T15:37:32.653394Z","iopub.execute_input":"2024-03-24T15:37:32.654369Z","iopub.status.idle":"2024-03-24T15:37:32.659115Z","shell.execute_reply.started":"2024-03-24T15:37:32.654327Z","shell.execute_reply":"2024-03-24T15:37:32.658276Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Initialize and train the neural network\nmodel_scratch = NeuralNetwork(input_size, hidden_size, output_size)\nmodel_scratch.train(X_train, y_train_one_hot, learning_rate=0.01, epochs=200)\n\n# Evaluation\ndef accuracy(y_true, y_pred):\n    return np.mean(y_true == y_pred)\n\npredictions = np.argmax(model_scratch.forward(X_test), axis=1)\ntrue_labels = np.argmax(y_test_one_hot, axis=1)\nprint(f\"Accuracy of scratch model: {accuracy(true_labels, predictions)}\")","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-03-24T15:37:33.681182Z","iopub.execute_input":"2024-03-24T15:37:33.681856Z","iopub.status.idle":"2024-03-24T15:39:44.150099Z","shell.execute_reply.started":"2024-03-24T15:37:33.681824Z","shell.execute_reply":"2024-03-24T15:39:44.148804Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Epoch 0: Loss 2.302292111684418\nEpoch 5: Loss 2.301628838851462\nEpoch 10: Loss 2.3009647929699053\nEpoch 15: Loss 2.3002986938137977\nEpoch 20: Loss 2.299629261608191\nEpoch 25: Loss 2.2989552142858996\nEpoch 30: Loss 2.2982752647807048\nEpoch 35: Loss 2.297588118353069\nEpoch 40: Loss 2.2968924699448103\nEpoch 45: Loss 2.2961870015596375\nEpoch 50: Loss 2.295470379666884\nEpoch 55: Loss 2.2947412526261606\nEpoch 60: Loss 2.2939982481311785\nEpoch 65: Loss 2.2932399706713857\nEpoch 70: Loss 2.2924649990105563\nEpoch 75: Loss 2.2916718836819756\nEpoch 80: Loss 2.2908591445003346\nEpoch 85: Loss 2.2900252680909885\nEpoch 90: Loss 2.2891687054377634\nEpoch 95: Loss 2.2882878694510445\nEpoch 100: Loss 2.287381132558441\nEpoch 105: Loss 2.2864468243209335\nEpoch 110: Loss 2.285483229077983\nEpoch 115: Loss 2.2844885836257216\nEpoch 120: Loss 2.2834610749329736\nEpoch 125: Loss 2.2823988379005065\nEpoch 130: Loss 2.2812999531695803\nEpoch 135: Loss 2.28016244498655\nEpoch 140: Loss 2.278984279130958\nEpoch 145: Loss 2.2777633609152823\nEpoch 150: Loss 2.276497533265213\nEpoch 155: Loss 2.2751845748900887\nEpoch 160: Loss 2.2738221985538565\nEpoch 165: Loss 2.2724080494577197\nEpoch 170: Loss 2.2709397037464236\nEpoch 175: Loss 2.2694146671509428\nEpoch 180: Loss 2.267830373781196\nEpoch 185: Loss 2.2661841850833073\nEpoch 190: Loss 2.264473388976851\nEpoch 195: Loss 2.2626951991884843\nAccuracy of scratch model: 0.6385\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}